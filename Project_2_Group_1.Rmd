---
title: "Kernel Based Smoothers"
author: "The Gang Boss"
date: "2024-04-16"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Don't need to change this, should already set your directory the file is in. 
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## Intro Slide



## Explanation
```{r}
#Kernel regression
# obtained from https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844
# We can use some different data but the way kernel regression works
datn <- data.frame(Area = c(11,22,33,44,50,56,67,70,78,89,90,100),       
                   RiverFlow = c(2337,2750,2301,2500,1700,2100,1100,1750,1000,1642, 2000,1932))                                 
x <- datn$Area
y <- datn$RiverFlow
#function to calculate Gaussian kernel
# Can use other types of kernels
gausinKernel <- function(x,b){
  K <- (1/((sqrt(2*pi))))*exp(-0.5 *(x/b)^2)
  return(K)
}

# Higher bandwidth smoother flatter curve
# Lower bandwidth more fit, can lead to overfitting
# Basically represents the standard deviation in a 
# kernel function
b <- 10 #bandwidth

# The x range the curve will have 
kdeEstimateyX <- seq(5,110,1)
ykernel <- NULL
for(xesti in kdeEstimateyX){
  xx <-  xesti - x
  K <-gausinKernel(xx,b)
  Ksum <- sum(K)
  weight <- K/Ksum
  yk <- sum(weight*y)
  xkyk <- c(xesti,yk)
  ykernel <- rbind(ykernel,xkyk)
}
plot(x,y,xlab = "Area", ylab = "Flow", col = 'blue', cex = 2)
lines(ykernel[,1],ykernel[,2], col = 'red', lwd = 2)

# This is just the slope values
# ykernel


```


## Plots
For these plots we will use our own crime data we find
```{r}
# define function and data
set.seed(1)
n <- 101
x <- seq(0, 1, length.out = n)
#x <- sort(runif(n,min = 0, max = 1))
fx <- sin(2 * pi * x)
# The y value is given some random error with variance of .5
# run plot(x,y) to see original data plot
y <- fx + rnorm(n, sd = 0.5)

# define x* and color for window
xstar <- 0.3
cols <- rgb(190/255, 190/255, 190/255, alpha = 0.5)

# set-up 2 x 2 subplot
par(mfrow = c(2,2))

# loop through h = c(1, 2, 3, 4) / 60
# sds of 1/60 1/30 1/20 and 1/15
for(h in c(1:4)/60){
  
  # plot data and true function
  plot(x, y, main = paste0("h = ", h * 60, "/60"), ylim = c(-2.5, 2.5),
       cex.lab = 1.5, cex.axis = 1.25)
  lines(x, fx, col = "blue", lwd = 2)
  
  # plot 99% window
  window <- c(xstar - 3 * h, xstar + 3 * h)
  rect(window[1], -3, window[2], 3, col = cols)
  
  # define weights
  w <- dnorm((x - xstar) / h) 
  w <- w / sum(w)
  
  # plot estimate
  ystar <- sum(y * w)
  points(xstar, ystar, pch = 17, col = "red", cex = 1)
  
  # add legend
  # legend("topright", legend = c("data", "truth"),
  #        pch = c(1, NA), lty = c(NA, 1), col = c("black", "blue"), bty = "n")
  # legend("bottomright", legend = c("estimate", "99% area"),
  #        pch = c(17, 15), col = c("red", "gray"), bty = "n")
  # 
}
```


## Animation
```{r}
library(animation)
h <- 1/60
set.seed(1)
n <- 101
x <- seq(0, 1, length.out = n)
fx <- sin(2 * pi * x)

y <- fx + rnorm(n, sd = 0.5)
ystarList <- c()
points <- c()
for(point in seq(0,1,length.out=n)){
  
  
 
  
  
  # plot 99% window
  window <- c(point - 3 * h, point + 3 * h)
 
  
  # define weights
  w <- dnorm((x - point) / h) 
  w <- w / sum(w)
  
  # plot estimate
  ystar <- sum(y * w)
  ystarList <- c(ystarList, ystar)
  points <- c(points, point)
  
  plot(x, y, main = paste0("h = ", h * 60, "/60"), ylim = c(-2.5, 2.5),
       cex.lab = 1.5, cex.axis = 1.25)
  lines(x, fx, col = "blue", lwd = 2)
  rect(window[1], -3, window[2], 3, col = cols)
  points(point, ystar, pch = 17, col = "red", cex = 1)
  lines(points, ystarList, col = "red", lwd=1)
  
  # add legend
  # legend("topright", legend = c("data", "truth"),
  #        pch = c(1, NA), lty = c(NA, 1), col = c("black", "blue"), bty = "n")
  # legend("bottomright", legend = c("estimate", "99% area"),
  #        pch = c(17, 15), col = c("red", "gray"), bty = "n")
  # 
}

```


## References
https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844

https://teazrq.github.io/SMLR/kernel-smoothing.html

https://bowtiedraptor.substack.com/p/kernel-regression

